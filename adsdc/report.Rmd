---
title: "Ayasid Data Challenge"
author: "Yan Wang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc}
---

Vignettes are long form documentation commonly included in packages. Because they are part of the distribution of the package, they need to be as compact as possible. The `html_vignette` output type provides a custom style sheet (and tweaks some options) to ensure that the resulting html is as small as possible. The `html_vignette` format:

- Never uses retina figures
- Has a smaller default figure size
- Uses a custom CSS stylesheet instead of the default Twitter Bootstrap style
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=5, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE)
```
## Data Overview
First take a first look at the data set.

```{r, echo=FALSE, results='asis'}
setwd('E:\\my work\\Github\\DataLab\\adsdc')
library(reshape2)
library(ggplot2)
library(GGally)
library(dplyr)
library(ggfortify)# ggplot PCA plotting
library(e1071)    # SVM library
library(randomForest)
library(DMwR)
raw<-read.csv("dataset_challenge_one.tsv",sep="\t", header=TRUE)
knitr::kable(head(raw[,c(1:3,(ncol(raw)-2):ncol(raw))], 5))
```

### Remove missing value
Check the completed records and remove the rows with missing value. There are 2 rows contain NA value which is relativly safe to discard.
```{r include=FALSE}
sum(!complete.cases(raw))
df<-raw[complete.cases(raw),]
nrow(df)
```
After removing the missing value, there are 270 completed records for analysis.

### Check the correlation ###
Looking at the relationship among variables would help to understand the data. The correlation between variables are calculated here. Sometimes the model require the factors are independent from each other, in that case we want to reduce the colinearity by removing highly correlated variables.

```{r}
var_col<-which(names(df)!="class")    # columns of variables
class_col<-which(names(df)=="class")  # column of class
# Overview:: Variable Correlation
mcor<-cor(df[, var_col])
melt_mcor<-melt(mcor)
melt_mcor<-melt_mcor[order(melt_mcor$value),]

# Plot the correlation distribution
p_histogram_correlation<-ggplot(melt_mcor, aes(x=value)) + 
  geom_histogram()+
  labs(x = "Correlation")+
  ggtitle("Distribution of Attribute Correlation")
p_histogram_correlation
```
According to the distribution of correlation, most of correlations are around 0 and in bell shape. It suggests most of the variables are relatively independent from each other. However there are `r nrow(subset(melt_mcor, value>=0.9 & Var1!=Var2))/2` pair of variables are highly correlated which correlation is >0.9. Probably we would like to take a further look to determine whether they are might be an issue or not. From a general perspective, it looks good since the number is few. 

### Outlier detection
Local outlier factor (LOF) is used as outlier detection methods. The method defines k nearest neighbors as the locality of a given point. The distance among locality is used to estimate the local density. The points with lower density are more like outliers.

```{r echo=FALSE,results="hide"}

outlier.scores <- lofactor(df[, var_col], k=5)
plot(density(outlier.scores),main="outlier score distribution")
outliers <- order(outlier.scores, decreasing=T)[1:10]
print(outliers)
labels<-1:nrow(df)
labels[-outliers] <- "."
fit <- prcomp(df[,var_col], cor=TRUE)
outlier_df<-df
outlier_df$class<-ifelse(labels>=1,-1,outlier_df$class)
outlier_df$class<-as.factor(outlier_df$class)
autoplot(fit, data =outlier_df, colour = 'class')
```

`r outliers` are the top 5 points in outlier detection. Next step, use field knowledge of the data and take a further look at those points, then decide whether to discard.

I am usually very cautious to do outlier detection. Because sometimes it is hard to tell whether an exotic point is a true outlier or just with some valuable information. Only with the good knowledge of variables, outlier detection makes more sense. So here I won't remove those "outliers".


## PCA
```{r echo=FALSE,results="hide"}
fit <- prcomp(df[,var_col], cor=TRUE)
summary(fit) # print variance accounted for 
fit$rotation[,1:2]
plot(fit,type="lines") # scree plot 
fit$scores # the principal components
autoplot(fit, data=df, colour = 'class')
df.pca<-data.frame((fit$x)[,1:2],class=df[,class_col])
corr.pca<-cor(df.pca)
df.pca$class<-as.factor(df.pca$class)
ggpairs(data= df.pca, mapping=aes(color=class))
```

### Relationship between variables and class
```{r echo=FALSE}
## correaltion between each var and class
varClassCorr<-sapply(df[,var_col], function(x) cor(x,as.numeric(df$class)))
top.var<-order(abs(varClassCorr), decreasing = T)[1:10]
var.sig<-df[,c(order(abs(varClassCorr), decreasing = T)[1:2],class_col)]
```
Using correlation between variable and class as the statistics of its significance. 
```{r echo=FALSE,results="hide"}
plot(varClassCorr[order(abs(varClassCorr), decreasing = T)[1:10]], main="Top 10 variable correlated to class")
```

Variable `r varClassCorr[order(abs(varClassCorr), decreasing = T)[1:2]]` are the top 2 variable correlated to `class`. The following charts shows the relationship between Variable `r varClassCorr[order(abs(varClassCorr), decreasing = T)[1:2]]` and class
```{r echo=FALSE,results="hide"}
var.sig$class<-as.factor(var.sig$class)
ggpairs(data=var.sig,mapping=aes(color=class) )
```

## Relationship between variables and PC1
```{r echo=FALSE}
# correaltion between each var and PC1
PC1=fit$x[,1]
varPC1Corr<-sapply(df[,var_col], function(x) cor(x,PC1))
order(abs(varPC1Corr), decreasing = T)[1:2] # top 2 variable correlated to PC1
varPC.sig<-cbind(df[,c(order(abs(varPC1Corr), decreasing = T)[1:2])],PC1=PC1) 
```

```{r echo=FALSE}
# top 10 variable correlated to PC1
plot(varPC1Corr[order(abs(varPC1Corr), decreasing = T)[1:10]],main="Top 10 variable correlated to PC1") 
```

```{r echo=FALSE}
# relationship between top 2 variable an PC1
ggpairs(data=varPC.sig)
```

## Classification
The data set has more variables than observations. A lot of superivsed learning methods have difficulty to handle high dimentional data which causes serious overfitting. Meanwhile, we are not sure about lineariy of the real system, so most of the linear methods should be rejected. Thus, SVM and random forest is chosen here to do the classifiction. Both of the methods can handle high dimentional data well , especially when variables >> observations. And also random forest is famous for its robustness on different messy data. So it is the one worth trying most of the time.

### Separate training and testing
Break the raw data into training set(75%) and testing set (25%). In most of the case, there is no need to have a test if cross-validation is used to estimated the model error. The purpose here is mimicking the reality that the model will be applied to some points after being built. Also the test set here is a good second check.
```{r echo=FALSE}
## Use 75% of the sample as training set
smp_size <- floor(0.75 * nrow(df))
set.seed(123)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]

# Separate variables and class
x<-subset(train,select=-class)
y<-train[,"class"]
x.test<-subset(test,select=-class)
y.test<-test[,"class"]
```

### SVM
SVM is designed to dealing with high dimentional data and non-linear system. It has very good performance in wide range of scenarios when data is not highly sknewed.  However, when the number of variables is much higher than the observations, SVM could be overfitted. Thus variable/feature selection is important in this case.
First, implement a simple SVM with tunned parameters:
```{r echo=FALSE}
# Classifier 1: SVM 
# SVM is good for when # of variables > # of records
svm_model <- svm(class ~., data=train, type="C-classification",cross=10)
svm_test<-predict(svm_model,x.test)
# Tune SVM
svm_tune <- tune(svm, train.x=x, train.y=y, kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
# Using the best cost and gamma to build SVM again
svm_model_tuned<-svm(class ~., data=train, type="C-classification", kernel="radial", cost=svm_tune$best.parameters$cost,gamma=svm_tune$best.parameters$gamma, cross=10)
svm_tune
```

The estimated error is `r 1-svm_model_tuned$tot.accuracy/100` which improves very little. And take a look at the accuracy by class: 
```{r echo=FALSE}
svm_pred_tuned<-predict(svm_model_tuned,x)
svm_tuned_test<-predict(svm_model_tuned,x.test)
knitr::kable(table(svm_tuned_test, y.test))
```

Test error: `r mean(svm_tuned_test == y.test)`
Model estimated error (by 10-fold CV): `r 1-svm_model_tuned$tot.accuracy/100`

#### Variable Selection
The number of variables is much larger than observations which may overfit the model if use all of them. A simple forward variable selection method is implemented to select the subset from training data set. Here are the steps:

1. Sort the variables by their correlation to `class` in descending order;
1. Start from the first variable which is the one with the highest correlation;
1. Train the SVM model with the subset of variables;
1. Tune the SVM model and estimated the error;
1. Add another variable into the training set and go back to #3;
1. Pick the subset with the lowest est. error.

Here we pick the best subset from the top 10 variables in terms of correlation to class.

```{r echo=FALSE, results='hide'}
# Order by the class correlation
var.sort<-order(abs(varClassCorr), decreasing = T)
best.performance=1
svm_best_tune = svm_tune
best.trainSet = train
for (v in 1:10){
  trainSet<- train[,var.sort[1:v]]
  svm_tune <- tune(svm, train.x=trainSet, train.y=y, kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
  if(svm_tune$best.performance < best.performance){
    best.performance = svm_tune$best.performance
    svm_best_tune = svm_tune
    best.trainSet = trainSet
  } 
}
head(best.trainSet)
svm_best_model<-svm(class ~., data=cbind(best.trainSet, class=y), type="C-classification", kernel="radial", cost=svm_best_tune$best.parameters$cost,gamma=svm_best_tune$best.parameters$gamma, cross=10)
svm_best_model_test<-predict(svm_best_model,x.test)
```

`r colnames(best.trainSet)` are selected. And here is the result of using the new training set

- estimated error: `r svm_best_tune$best.performance`
- test error: `r 1-mean(svm_best_model_test == y.test)`

### Random forest
Based on the random forest official website (http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#remarks), Random forests is very robust against large number of variables. It is very hard to make RF overfit.  Thus I don't use any variable selection techniques. Meanwhile, random forest uses bootstraping, so there is also no need for cross-validation or separate test set. The only reason for using test set here is to give a second check.

```{r echo=FALSE, ,results="hide"}
rf_model <- randomForest(as.factor(class) ~ ., data=train, importance=TRUE, ntree=2000)
plot(rf_model)
pred_rf<-predict(rf_model,x)
rf_model_test<-predict(rf_model,x.test)
```
From the fiture above, the predict error get stable as number of trees going up.
When Here is the performance summary of random forest
```{r echo=FALSE}
rf_model
```


### Conclusion
Based on the test set accuracy and cross validation, the random forest 
SVM

- estimated error: `r 1-svm_model_tuned$tot.accuracy/100`
- test error: `r mean(svm_tuned_test == y.test)`

SVM - Simple Forawrd Selection

- estimated error: `r svm_best_tune$best.performance`
- test error: `r 1-mean(svm_best_model_test == y.test)`

Randomforest

- estimated error: `r median((rf_model$err.rate)[,1])`
- test error: `r 1-mean(rf_model_test == y.test)`

The figure sizes have been customised so that you can easily put two images side-by-side. 

```{r, fig.show='hold'}
plot(1:10)
plot(10:1)
```

You can enable figure captions by `fig_caption: yes` in YAML:

    output:
      rmarkdown::html_vignette:
        fig_caption: yes

Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**.

## More Examples

You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`.

```{r, echo=FALSE, results='asis'}
knitr::kable(head(mtcars, 10))
```

Also a quote using `>`:

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))
